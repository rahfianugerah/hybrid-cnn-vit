{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid CNN–ViT Training Notebook (Updated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.6 environment at: /usr/local\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 17ms\u001b[0m\u001b[0m\r\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%uv pip install optuna timm torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43662a74",
      "metadata": {},
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import optuna\n",
        "from optuna.trial import Trial\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
        "\n",
        "MODEL_DIR = os.environ.get(\"MODEL_DIR\", \"/mnt/vit-hybrid-optuna\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89005b72",
      "metadata": {},
      "source": [
        "## Hyrbid CNN-ViT Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-03 03:57:46,228] A new study created in memory with name: no-name-58c5b714-23f9-444d-be19-350200d6a5c7\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using Device: cuda\n",
            "[INFO] Hybrid CNN-ViT Architecture - Optuna Tuning\n",
            "[INFO] \n",
            "[1/2] Running Optuna Hyperparameter Optimization...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|                                                                               | 0.00/170M [00:00<?, ?B/s]\r  0%|▎                                                                      | 688k/170M [00:00<00:24, 6.85MB/s]\r  5%|███▏                                                                  | 7.70M/170M [00:00<00:03, 43.8MB/s]\r 10%|███████                                                               | 17.1M/170M [00:00<00:02, 66.7MB/s]\r 15%|██████████▊                                                           | 26.3M/170M [00:00<00:01, 76.6MB/s]\r 21%|██████████████▍                                                       | 35.2M/170M [00:00<00:01, 80.7MB/s]\r 26%|██████████████████▏                                                   | 44.4M/170M [00:00<00:01, 84.8MB/s]\r 31%|█████████████████████▉                                                | 53.5M/170M [00:00<00:01, 86.6MB/s]\r 37%|█████████████████████████▉                                            | 63.1M/170M [00:00<00:01, 89.5MB/s]\r 43%|█████████████████████████████▊                                        | 72.5M/170M [00:00<00:01, 91.1MB/s]\r 48%|█████████████████████████████████▌                                    | 81.7M/170M [00:01<00:00, 90.0MB/s]\r 53%|█████████████████████████████████████▏                                | 90.7M/170M [00:01<00:00, 85.8MB/s]\r 58%|████████████████████████████████████████▊                             | 99.3M/170M [00:01<00:00, 81.2MB/s]\r 63%|████████████████████████████████████████████▊                          | 108M/170M [00:01<00:00, 78.8MB/s]\r 68%|████████████████████████████████████████████████▏                      | 116M/170M [00:01<00:00, 79.6MB/s]\r 73%|███████████████████████████████████████████████████▍                   | 124M/170M [00:01<00:00, 79.0MB/s]\r 77%|██████████████████████████████████████████████████████▊                | 132M/170M [00:01<00:00, 78.9MB/s]\r 82%|██████████████████████████████████████████████████████████             | 140M/170M [00:01<00:00, 78.4MB/s]\r 86%|█████████████████████████████████████████████████████████████▍         | 147M/170M [00:01<00:00, 78.7MB/s]\r 91%|████████████████████████████████████████████████████████████████▋      | 155M/170M [00:01<00:00, 77.8MB/s]\r 96%|███████████████████████████████████████████████████████████████████▉   | 163M/170M [00:02<00:00, 76.4MB/s]\r100%|███████████████████████████████████████████████████████████████████████| 170M/170M [00:02<00:00, 78.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|                                                                              | 0.00/20.5M [00:00<?, ?B/s]\r  4%|██▌                                                                   | 768k/20.5M [00:00<00:02, 7.43MB/s]\r 35%|████████████████████████▍                                            | 7.25M/20.5M [00:00<00:00, 42.1MB/s]\r 74%|███████████████████████████████████████████████████                  | 15.1M/20.5M [00:00<00:00, 60.1MB/s]\r100%|█████████████████████████████████████████████████████████████████████| 20.5M/20.5M [00:00<00:00, 57.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Epoch 1: Train Acc=51.63%, Test Acc=72.38%, Test F1=0.7197, Test Loss=1.1272\n",
            "[INFO] Epoch 2: Train Acc=66.82%, Test Acc=78.45%, Test F1=0.7821, Test Loss=0.9955\n",
            "[INFO] Epoch 3: Train Acc=71.05%, Test Acc=79.98%, Test F1=0.7969, Test Loss=0.9528\n",
            "[INFO] Epoch 4: Train Acc=73.86%, Test Acc=81.92%, Test F1=0.8170, Test Loss=0.9184\n",
            "[INFO] Epoch 5: Train Acc=75.50%, Test Acc=82.61%, Test F1=0.8241, Test Loss=0.9016\n",
            "[INFO] Epoch 6: Train Acc=77.33%, Test Acc=83.34%, Test F1=0.8327, Test Loss=0.8783\n",
            "[INFO] Epoch 7: Train Acc=78.06%, Test Acc=84.48%, Test F1=0.8440, Test Loss=0.8535\n",
            "[INFO] Epoch 8: Train Acc=79.05%, Test Acc=84.79%, Test F1=0.8464, Test Loss=0.8483\n",
            "[INFO] Epoch 9: Train Acc=79.80%, Test Acc=85.28%, Test F1=0.8521, Test Loss=0.8421\n",
            "[INFO] Epoch 10: Train Acc=80.06%, Test Acc=85.30%, Test F1=0.8518, Test Loss=0.8422\n",
            "[INFO] Epoch 11: Train Acc=79.74%, Test Acc=85.14%, Test F1=0.8504, Test Loss=0.8416\n",
            "[INFO] Epoch 12: Train Acc=80.27%, Test Acc=85.13%, Test F1=0.8503, Test Loss=0.8426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-03 04:01:31,468] Trial 0 finished with value: 85.30000448226929 and parameters: {'backbone': 'efficientnet', 'embed_dim': 128, 'num_heads': 4, 'num_layers': 4, 'mlp_ratio': 2.3883077120553216, 'dropout': 0.18282095801199652, 'lr': 0.0006486992336048424, 'batch_size': 256}. Best is trial 0 with value: 85.30000448226929.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Epoch 1: Train Acc=51.58%, Test Acc=71.02%, Test F1=0.7054, Test Loss=1.1569\n",
            "[INFO] Epoch 2: Train Acc=65.72%, Test Acc=76.58%, Test F1=0.7634, Test Loss=1.0304\n",
            "[INFO] Epoch 3: Train Acc=70.13%, Test Acc=79.40%, Test F1=0.7926, Test Loss=0.9714\n",
            "[INFO] Epoch 4: Train Acc=72.75%, Test Acc=80.83%, Test F1=0.8062, Test Loss=0.9407\n",
            "[INFO] Epoch 5: Train Acc=74.66%, Test Acc=81.81%, Test F1=0.8170, Test Loss=0.9149\n",
            "[INFO] Epoch 6: Train Acc=75.83%, Test Acc=82.79%, Test F1=0.8272, Test Loss=0.8873\n",
            "[INFO] Epoch 7: Train Acc=77.38%, Test Acc=83.00%, Test F1=0.8283, Test Loss=0.8818\n",
            "[INFO] Epoch 8: Train Acc=78.00%, Test Acc=83.45%, Test F1=0.8329, Test Loss=0.8684\n",
            "[INFO] Epoch 9: Train Acc=78.65%, Test Acc=83.94%, Test F1=0.8380, Test Loss=0.8632\n",
            "[INFO] Epoch 10: Train Acc=78.84%, Test Acc=83.91%, Test F1=0.8377, Test Loss=0.8608\n",
            "[INFO] Epoch 11: Train Acc=78.93%, Test Acc=83.96%, Test F1=0.8383, Test Loss=0.8604\n",
            "[INFO] Epoch 12: Train Acc=79.09%, Test Acc=83.88%, Test F1=0.8372, Test Loss=0.8622\n",
            "[INFO] Epoch 13: Train Acc=79.02%, Test Acc=83.82%, Test F1=0.8367, Test Loss=0.8611\n",
            "[INFO] Epoch 14: Train Acc=78.91%, Test Acc=84.19%, Test F1=0.8412, Test Loss=0.8592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-03 04:06:09,902] Trial 1 finished with value: 84.19000506401062 and parameters: {'backbone': 'efficientnet', 'embed_dim': 256, 'num_heads': 12, 'num_layers': 5, 'mlp_ratio': 3.746607213994121, 'dropout': 0.16402878887680009, 'lr': 0.0004365165085324784, 'batch_size': 256}. Best is trial 0 with value: 85.30000448226929.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Epoch 15: Train Acc=78.97%, Test Acc=83.93%, Test F1=0.8381, Test Loss=0.8647\n",
            "[INFO] Epoch 1: Train Acc=49.70%, Test Acc=70.15%, Test F1=0.6954, Test Loss=1.1815\n",
            "[INFO] Epoch 2: Train Acc=63.38%, Test Acc=75.18%, Test F1=0.7489, Test Loss=1.0640\n",
            "[INFO] Epoch 3: Train Acc=67.76%, Test Acc=77.50%, Test F1=0.7731, Test Loss=1.0149\n",
            "[INFO] Epoch 4: Train Acc=70.26%, Test Acc=79.15%, Test F1=0.7896, Test Loss=0.9729\n",
            "[INFO] Epoch 5: Train Acc=71.83%, Test Acc=80.50%, Test F1=0.8030, Test Loss=0.9455\n",
            "[INFO] Epoch 6: Train Acc=73.66%, Test Acc=81.67%, Test F1=0.8142, Test Loss=0.9169\n",
            "[INFO] Epoch 7: Train Acc=74.82%, Test Acc=82.02%, Test F1=0.8193, Test Loss=0.9069\n",
            "[INFO] Epoch 8: Train Acc=75.34%, Test Acc=82.57%, Test F1=0.8243, Test Loss=0.8963\n",
            "[INFO] Epoch 9: Train Acc=76.11%, Test Acc=82.85%, Test F1=0.8272, Test Loss=0.8923\n",
            "[INFO] Epoch 10: Train Acc=76.22%, Test Acc=83.05%, Test F1=0.8291, Test Loss=0.8903\n",
            "[INFO] Epoch 11: Train Acc=76.49%, Test Acc=82.92%, Test F1=0.8278, Test Loss=0.8911\n",
            "[INFO] Epoch 12: Train Acc=76.23%, Test Acc=83.10%, Test F1=0.8298, Test Loss=0.8859\n",
            "[INFO] Epoch 13: Train Acc=76.38%, Test Acc=83.17%, Test F1=0.8307, Test Loss=0.8871\n",
            "[INFO] Epoch 14: Train Acc=76.43%, Test Acc=83.14%, Test F1=0.8304, Test Loss=0.8841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-03 04:11:26,072] Trial 2 finished with value: 83.17000269889832 and parameters: {'backbone': 'efficientnet', 'embed_dim': 256, 'num_heads': 6, 'num_layers': 5, 'mlp_ratio': 3.8810242636868932, 'dropout': 0.21411406441653213, 'lr': 0.0002110510081711246, 'batch_size': 128}. Best is trial 0 with value: 85.30000448226929.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Epoch 15: Train Acc=76.41%, Test Acc=83.08%, Test F1=0.8292, Test Loss=0.8822\n",
            "[INFO] Optimization Results:\n",
            "[INFO] Best Accuracy: 85.30%\n",
            "[INFO] Best Hyperparameters: {'backbone': 'efficientnet', 'embed_dim': 128, 'num_heads': 4, 'num_layers': 4, 'mlp_ratio': 2.3883077120553216, 'dropout': 0.18282095801199652, 'lr': 0.0006486992336048424, 'batch_size': 256}\n",
            "[INFO] ✓ Saved Optuna Best Summary to: /mnt/vit-hybrid-optuna/optuna_best.json\n",
            "[INFO] \n",
            "[2/2] Training final model with best hyperparameters...\n",
            "[INFO] \n",
            "Training Final Model for 30 Epochs\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=72.69%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=78.15%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=80.03%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=81.62%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=82.72%)\n",
            "[INFO] Epoch 5/30: Train Acc=74.88%, Test Acc=82.72%, Test F1=0.8249, Test Loss=0.9042\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=83.32%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=84.19%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=84.38%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=84.98%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=85.04%)\n",
            "[INFO] Epoch 10/30: Train Acc=80.21%, Test Acc=85.04%, Test F1=0.8498, Test Loss=0.8447\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=85.36%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=86.30%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=86.46%)\n",
            "[INFO] Epoch 15/30: Train Acc=82.89%, Test Acc=86.46%, Test F1=0.8636, Test Loss=0.8187\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=86.77%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=86.95%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=87.29%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=87.35%)\n",
            "[INFO] Epoch 20/30: Train Acc=84.95%, Test Acc=87.35%, Test F1=0.8731, Test Loss=0.7989\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=87.61%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=87.80%)\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=88.00%)\n",
            "[INFO] Epoch 25/30: Train Acc=86.40%, Test Acc=88.00%, Test F1=0.8797, Test Loss=0.7965\n",
            "[INFO] ✓ Saved Best state_dict to: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth (acc=88.01%)\n",
            "[INFO] Epoch 30/30: Train Acc=86.96%, Test Acc=87.85%, Test F1=0.8782, Test Loss=0.7948\n",
            "[INFO] Final Results:\n",
            "[INFO] Best Test Accuracy: 88.01%\n",
            "[INFO] Target Achieved: ✗ NO\n",
            "[INFO] \n",
            "Model Parameters: 4,709,770 (Trainable: 4,709,770)\n",
            "[INFO] \n",
            "Model State Saved at: /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth\n",
            "[INFO] Transfer Learning Utilities\n",
            "[INFO] ✓ Full Model Saved to: /mnt/vit-hybrid-optuna/hybrid_cnn_vit_full.pth\n",
            "[INFO] Transfer Learning Guide\n",
            "[INFO] \n",
            "        To use this model on other datasets:\n",
            "\n",
            "        1. Similiar Datasets (e.g., CIFAR-100, STL-10, Tiny ImageNet):\n",
            "           - Load pretrained model with freeze_backbone=False\n",
            "           - Fine-tune entire model with lower learning rate\n",
            "\n",
            "        2. Differents Domains (e.g., Medical images, Satellite imagery):\n",
            "           - Load pretrained model with freeze_backbone=True\n",
            "           - Train transformer layers and head first\n",
            "\n",
            "        3. Different Image Sizes:\n",
            "           - Resize images to 32x32 OR\n",
            "           - Modify patch_embed layer for new resolution and let pos-enc interpolate\n",
            "    \n",
            "[INFO] Files Saved:\n",
            "[INFO] Best state dict       : /mnt/vit-hybrid-optuna/best_hybrid_cnn_vit.pth\n",
            "[INFO] Full checkpoint (+hps) : /mnt/vit-hybrid-optuna/hybrid_cnn_vit_full.pth\n",
            "[INFO] Finetuned state dict   : /mnt/vit-hybrid-optuna/finetuned_model.pth\n",
            "[INFO] Optuna best summary    : /mnt/vit-hybrid-optuna/optuna_best.json\n"
          ]
        }
      ],
      "source": [
        "# Paths & IO (Modal-friendly)\n",
        "MODEL_DIR = os.environ.get(\"MODEL_DIR\", \"/mnt/vit-hybrid-optuna\")  # On Modal, mount a Volume at /mnt/vit-hybrid-optuna\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "BEST_STATE_PATH = os.path.join(MODEL_DIR, \"best_hybrid_cnn_vit.pth\")\n",
        "FULL_CKPT_PATH = os.path.join(MODEL_DIR, \"hybrid_cnn_vit_full.pth\")\n",
        "FINETUNED_PATH = os.path.join(MODEL_DIR, \"finetuned_model.pth\")\n",
        "STUDY_JSON_PATH = os.path.join(MODEL_DIR, \"optuna_best.json\")\n",
        "\n",
        "# Reproducibility & Device\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using Device: {device}\")\n",
        "\n",
        "# Metrics Setup\n",
        "num_classes = 10\n",
        "avg_type = 'macro'  # 'macro' fair to all classes, 'micro' = global accuracy\n",
        "\n",
        "metric_collection = MetricCollection({\n",
        "    'accuracy': MulticlassAccuracy(num_classes=num_classes, average=avg_type),\n",
        "    'precision': MulticlassPrecision(num_classes=num_classes, average=avg_type),\n",
        "    'recall': MulticlassRecall(num_classes=num_classes, average=avg_type),\n",
        "    'f1_score': MulticlassF1Score(num_classes=num_classes, average=avg_type)\n",
        "}).to(device)\n",
        "\n",
        "# Data Loading\n",
        "def get_dataloaders(batch_size=128):\n",
        "    \"\"\"Load CIFAR-10 with augmentation\"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    \n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    \n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform_test)\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# CNN Feature Extractors\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    \"\"\"EfficientNet-B0 multi-scale feature extractor with taps [2, 4, 6].\"\"\"\n",
        "    def __init__(self, model_name='efficientnet_b0'):\n",
        "        super().__init__()\n",
        "        from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "        base_model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        self.features = base_model.features\n",
        "        # Channels at indices [2, 4, 6] for EfficientNet-B0\n",
        "        self.out_channels = [24, 80, 192]\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = []\n",
        "        for i, layer in enumerate(self.features):\n",
        "            x = layer(x)\n",
        "            if i in [2, 4, 6]:\n",
        "                feats.append(x)\n",
        "        return feats\n",
        "\n",
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    \"\"\"ResNet-18 multi-scale feature extractor returning [layer2, layer3, layer4].\"\"\"\n",
        "    def __init__(self, depth='resnet18'):\n",
        "        super().__init__()\n",
        "        from torchvision.models import resnet18, ResNet18_Weights\n",
        "        base_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.conv1 = base_model.conv1\n",
        "        self.bn1 = base_model.bn1\n",
        "        self.relu = base_model.relu\n",
        "        self.maxpool = base_model.maxpool\n",
        "        self.layer1 = base_model.layer1\n",
        "        self.layer2 = base_model.layer2\n",
        "        self.layer3 = base_model.layer3\n",
        "        self.layer4 = base_model.layer4\n",
        "        # We return [f2, f3, f4] → channels [128, 256, 512]\n",
        "        self.out_channels = [128, 256, 512]\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x); x = self.bn1(x); x = self.relu(x); x = self.maxpool(x)\n",
        "        f1 = self.layer1(x)  # 64 ch\n",
        "        f2 = self.layer2(f1) # 128 ch\n",
        "        f3 = self.layer3(f2) # 256 ch\n",
        "        f4 = self.layer4(f3) # 512 ch\n",
        "        return [f2, f3, f4]\n",
        "\n",
        "# Adaptive Patch Embedding\n",
        "class AdaptivePatchEmbedding(nn.Module):\n",
        "    \"\"\"Adaptive patch embedding with learnable pos-enc and class token.\"\"\"\n",
        "    def __init__(self, in_channels, embed_dim, max_img_size=8):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=1, stride=1)\n",
        "        self.max_patches = max_img_size * max_img_size\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.max_patches + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x)                  # (B, embed_dim, H, W)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, H*W, embed_dim)\n",
        "        num_patches = x.shape[1]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1+H*W, embed_dim)\n",
        "        pos_embed = self.pos_embed if (num_patches + 1 == self.pos_embed.shape[1]) \\\n",
        "            else self.interpolate_pos_encoding(H, W)\n",
        "        return x + pos_embed\n",
        "    \n",
        "    def interpolate_pos_encoding(self, H, W):\n",
        "        N = self.pos_embed.shape[1] - 1\n",
        "        class_pos_embed = self.pos_embed[:, 0:1]\n",
        "        patch_pos_embed = self.pos_embed[:, 1:]\n",
        "        orig_size = int(math.sqrt(N))\n",
        "        patch_pos_embed = patch_pos_embed.reshape(1, orig_size, orig_size, -1).permute(0, 3, 1, 2)\n",
        "        patch_pos_embed = F.interpolate(patch_pos_embed, size=(H, W), mode='bilinear', align_corners=False)\n",
        "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).reshape(1, -1, self.embed_dim)\n",
        "        return torch.cat([class_pos_embed, patch_pos_embed], dim=1)\n",
        "\n",
        "# Multi-Scale Feature Fusion\n",
        "class MultiScaleFeatureFusion(nn.Module):\n",
        "    \"\"\"Fuse features from multiple scales to a unified embed_dim.\"\"\"\n",
        "    def __init__(self, channels_list: List[int], out_channels: int):\n",
        "        super().__init__()\n",
        "        self.projections = nn.ModuleList([nn.Conv2d(ch, out_channels, 1) for ch in channels_list])\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Conv2d(out_channels * len(channels_list), out_channels, 1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "    def forward(self, features: List[torch.Tensor]):\n",
        "        assert len(features) == len(self.projections), \\\n",
        "            f\"Got {len(features)} features but {len(self.projections)} projections\"\n",
        "        target_size = features[-1].shape[2:]\n",
        "        aligned = []\n",
        "        for idx, (feat, proj) in enumerate(zip(features, self.projections)):\n",
        "            assert feat.shape[1] == proj.in_channels, \\\n",
        "                f\"Fusion input ch mismatch at idx {idx}: tensor has {feat.shape[1]}, proj expects {proj.in_channels}\"\n",
        "            feat = proj(feat)\n",
        "            if feat.shape[2:] != target_size:\n",
        "                feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
        "            aligned.append(feat)\n",
        "        fused = torch.cat(aligned, dim=1)\n",
        "        return self.fusion(fused)\n",
        "\n",
        "# Vision Transformer Encoder\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.attn_drop = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.proj_drop = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "# Hybrid CNN-ViT Model\n",
        "class HybridCNNViT(nn.Module):\n",
        "    \"\"\"Hybrid CNN-ViT with adaptive patch embedding and multi-scale fusion\"\"\"\n",
        "    def __init__(self, \n",
        "                 backbone='efficientnet',\n",
        "                 embed_dim=256,\n",
        "                 num_heads=8,\n",
        "                 num_layers=4,\n",
        "                 mlp_ratio=4.0,\n",
        "                 dropout=0.1,\n",
        "                 num_classes=10):\n",
        "        super().__init__()\n",
        "        if backbone == 'efficientnet':\n",
        "            self.backbone = EfficientNetFeatureExtractor()\n",
        "        else:\n",
        "            self.backbone = ResNetFeatureExtractor()\n",
        "        channels_list = self.backbone.out_channels  # single source of truth\n",
        "        \n",
        "        self.fusion = MultiScaleFeatureFusion(channels_list, embed_dim)\n",
        "        self.patch_embed = AdaptivePatchEmbedding(embed_dim, embed_dim, max_img_size=8)\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "                                     for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        fused = self.fusion(features)\n",
        "        x = self.patch_embed(fused)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]\n",
        "        logits = self.head(cls_token)\n",
        "        return logits\n",
        "\n",
        "# Training & Evaluation\n",
        "def train_epoch(model, loader, optimizer, criterion, device):\n",
        "    \"\"\"train_epoch function\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0; correct = 0; total = 0\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "# Modified evaluate function to use torchmetrics\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate function modified to use torchmetrics.\n",
        "    Now returns a dictionary containing all metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    metric_collection.reset() # Reset metrics at each evaluation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "\n",
        "            metric_collection.update(outputs, targets)\n",
        "\n",
        "    final_metrics = metric_collection.compute()\n",
        "    final_loss = total_loss / len(loader)\n",
        "\n",
        "    metrics_dict = {k: v.item() for k, v in final_metrics.items()}\n",
        "    metrics_dict['loss'] = final_loss\n",
        "\n",
        "    metrics_dict['accuracy'] = metrics_dict['accuracy'] * 100 \n",
        "    \n",
        "    return metrics_dict\n",
        "\n",
        "CHOICES_HEADS = [4, 6, 8, 12]\n",
        "\n",
        "def _effective_num_heads(embed_dim: int, trial_choice: int) -> int:\n",
        "    \"\"\"Map a trial choice to a valid head count given embed_dim (avoid dynamic value space).\"\"\"\n",
        "    valid = [h for h in CHOICES_HEADS if embed_dim % h == 0]\n",
        "    if not valid:\n",
        "        # Fallback: ensure at least 1 head (should not happen with provided embed_dim choices)\n",
        "        return 1\n",
        "    if trial_choice in valid:\n",
        "        return trial_choice\n",
        "    # Deterministic projection: pick the largest valid head <= trial_choice, else pick max(valid)\n",
        "    smaller_or_equal = [h for h in valid if h <= trial_choice]\n",
        "    return max(smaller_or_equal) if smaller_or_equal else max(valid)\n",
        "\n",
        "def objective(trial: Trial):\n",
        "    \"\"\"Optuna objective function\"\"\"\n",
        "    backbone = trial.suggest_categorical('backbone', ['efficientnet', 'resnet'])\n",
        "    embed_dim = trial.suggest_categorical('embed_dim', [128, 192, 256, 384])\n",
        "    \n",
        "    # Fixed categorical (no dynamic space). We project to a valid value after sampling.\n",
        "    num_heads_raw = trial.suggest_categorical('num_heads', CHOICES_HEADS)\n",
        "    num_heads = _effective_num_heads(embed_dim, num_heads_raw)\n",
        "    trial.set_user_attr('num_heads_effective', num_heads)\n",
        "\n",
        "    num_layers = trial.suggest_int('num_layers', 3, 6)\n",
        "    mlp_ratio = trial.suggest_float('mlp_ratio', 2.0, 4.0)\n",
        "    dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
        "    lr = trial.suggest_float('lr', 1e-4, 1e-3, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256])\n",
        "    \n",
        "    train_loader, test_loader = get_dataloaders(batch_size)\n",
        "    \n",
        "    model = HybridCNNViT(\n",
        "        backbone=backbone,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        num_layers=num_layers,\n",
        "        mlp_ratio=mlp_ratio,\n",
        "        dropout=dropout,\n",
        "        num_classes=10\n",
        "    ).to(device)\n",
        "    \n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    patience = 0\n",
        "    max_patience = 3\n",
        "    \n",
        "    for epoch in range(15): # Reduced for faster tuning\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "        test_acc = test_metrics[\"accuracy\"]\n",
        "        test_f1 = test_metrics[\"f1_score\"]\n",
        "        \n",
        "        scheduler.step()\n",
        "        trial.report(test_acc, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc; patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= max_patience:\n",
        "                break\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Acc={train_acc:.2f}%, \"\n",
        "              f\"Test Acc={test_acc:.2f}%, Test F1={test_f1:.4f}, Test Loss={test_metrics['loss']:.4f}\")\n",
        "              \n",
        "    return best_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Hybrid CNN-ViT Architecture - Optuna Tuning\")\n",
        "\n",
        "    print(\"\\n[1/2] Running Optuna Hyperparameter Optimization...\\n\")\n",
        "    study = optuna.create_study(\n",
        "        direction='maximize',\n",
        "        pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=5)\n",
        "    )\n",
        "    study.optimize(objective, n_trials = 3, timeout=3600)  # 10 trials or 1 hour\n",
        "\n",
        "    print(\"Optimization Results:\")\n",
        "    print(f\"Best Accuracy: {study.best_value:.2f}%\")\n",
        "    print(f\"Best Hyperparameters: {study.best_params}\")\n",
        "\n",
        "    # Persist Optuna best to JSON (optional, helpful on Modal)\n",
        "    try:\n",
        "        import json\n",
        "        with open(STUDY_JSON_PATH, \"w\") as f:\n",
        "            json.dump({\"best_value\": study.best_value, \"best_params\": study.best_params}, f, indent=2)\n",
        "        print(f\"✓ Saved Optuna Best Summary to: {STUDY_JSON_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not Write Study Summary: {e}\")\n",
        "\n",
        "    # Train final model with best hyperparameters\n",
        "    print(\"\\n[2/2] Training final model with best hyperparameters...\")\n",
        "    best_params = study.best_params\n",
        "    \n",
        "    # Derive effective heads again (consistent with objective)\n",
        "    num_heads_eff = _effective_num_heads(best_params['embed_dim'], best_params.get('num_heads', 8))\n",
        "\n",
        "    train_loader, test_loader = get_dataloaders(best_params['batch_size'])\n",
        "\n",
        "    final_model = HybridCNNViT(\n",
        "        backbone=best_params['backbone'],\n",
        "        embed_dim=best_params['embed_dim'],\n",
        "        num_heads=num_heads_eff,\n",
        "        num_layers=best_params['num_layers'],\n",
        "        mlp_ratio=best_params['mlp_ratio'],\n",
        "        dropout=best_params['dropout'],\n",
        "        num_classes=10\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(final_model.parameters(), \n",
        "                                  lr=best_params['lr'], \n",
        "                                  weight_decay=0.05)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    print(\"\\nTraining Final Model for 30 Epochs\")\n",
        "\n",
        "    for epoch in range(30):\n",
        "        train_loss, train_acc = train_epoch(final_model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        test_metrics = evaluate(final_model, test_loader, criterion, device)\n",
        "        test_acc = test_metrics[\"accuracy\"]\n",
        "        test_f1 = test_metrics[\"f1_score\"]\n",
        "\n",
        "        scheduler.step()\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            try:\n",
        "                torch.save(final_model.state_dict(), BEST_STATE_PATH)\n",
        "                print(f\"✓ Saved Best state_dict to: {BEST_STATE_PATH} (acc={best_acc:.2f}%)\")\n",
        "            except Exception as e:\n",
        "                print(f\"Save Failed: {e}\")\n",
        "        \n",
        "        if (epoch + 1) % 5 == 0:\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/30: Train Acc={train_acc:.2f}%, \"\n",
        "                  f\"Test Acc={test_acc:.2f}%, Test F1={test_f1:.4f}, Test Loss={test_metrics['loss']:.4f}\")\n",
        "\n",
        "    print(\"Final Results:\")\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"Target Achieved: {'✓ YES' if best_acc >= 90 else '✗ NO'}\")\n",
        "\n",
        "    # Model summary\n",
        "    total_params = sum(p.numel() for p in final_model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in final_model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nModel Parameters: {total_params:,} (Trainable: {trainable_params:,})\")\n",
        "    print(f\"\\nModel State Saved at: {BEST_STATE_PATH}\")\n",
        "\n",
        "    print(\"Transfer Learning Utilities\")\n",
        "\n",
        "    def save_full_model(model, hyperparams, path=FULL_CKPT_PATH):\n",
        "        \"\"\"Save complete model with hyperparameters for transfer learning\"\"\"\n",
        "        try:\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'hyperparameters': hyperparams,\n",
        "                'model_architecture': 'HybridCNNViT',\n",
        "                'training_info': {\n",
        "                    'dataset': 'CIFAR-10',\n",
        "                    'num_classes': 10,\n",
        "                    'best_accuracy': best_acc\n",
        "                }\n",
        "            }, path)\n",
        "            print(f\"✓ Full Model Saved to: {path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to Save Full Model: {e}\")\n",
        "\n",
        "    def load_pretrained_model(path=FULL_CKPT_PATH, num_classes_new=10, freeze_backbone=False):\n",
        "        \"\"\"\n",
        "        Load pretrained model and adapt for new dataset\n",
        "        \"\"\"\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        hyperparams = checkpoint['hyperparameters']\n",
        "        heads_eff = _effective_num_heads(hyperparams['embed_dim'], hyperparams.get('num_heads', 8))\n",
        "        model = HybridCNNViT(\n",
        "            backbone=hyperparams['backbone'],\n",
        "            embed_dim=hyperparams['embed_dim'],\n",
        "            num_heads=heads_eff,\n",
        "            num_layers=hyperparams['num_layers'],\n",
        "            mlp_ratio=hyperparams['mlp_ratio'],\n",
        "            dropout=hyperparams['dropout'],\n",
        "            num_classes=checkpoint['training_info']['num_classes']\n",
        "        ).to(device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        if num_classes_new != checkpoint['training_info']['num_classes']:\n",
        "            print(f\"Adapting Model: {checkpoint['training_info']['num_classes']} → {num_classes_new} classes\")\n",
        "            model.head = nn.Sequential(\n",
        "                nn.Linear(hyperparams['embed_dim'], hyperparams['embed_dim'] // 2),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(hyperparams['dropout']),\n",
        "                nn.Linear(hyperparams['embed_dim'] // 2, num_classes_new)\n",
        "            ).to(device)\n",
        "        if freeze_backbone:\n",
        "            print(\"Freezing CNN Backbone Layers...\")\n",
        "            for p in model.backbone.parameters(): p.requires_grad = False\n",
        "            for p in model.fusion.parameters(): p.requires_grad = False\n",
        "        return model, hyperparams\n",
        "\n",
        "    def fine_tune_on_new_dataset(model, train_loader, test_loader, \n",
        "                                 num_epochs = 5, lr=1e-4):\n",
        "        \"\"\"\n",
        "        Fine-tune pretrained model on new dataset\n",
        "        \"\"\"\n",
        "        print(f\"\\nFine-tuning for {num_epochs} Epochs with lr={lr}...\")\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=lr, \n",
        "            weight_decay=0.05\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        \n",
        "        best_ft_acc = 0.0\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "            \n",
        "            test_metrics = evaluate(model, test_loader, criterion, device)\n",
        "            test_acc = test_metrics[\"accuracy\"]\n",
        "            \n",
        "            scheduler.step()\n",
        "            if test_acc > best_ft_acc:\n",
        "                best_ft_acc = test_acc\n",
        "                try:\n",
        "                    torch.save(model.state_dict(), FINETUNED_PATH)\n",
        "                    print(f\"✓ Saved Finetuned state_dict to: {FINETUNED_PATH} (acc={best_ft_acc:.2f}%)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Save Failed: {e}\")\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{num_epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}%\")\n",
        "        print(f\"\\n✓ Fine-tuning Complete! Best Accuracy: {best_ft_acc:.2f}%\")\n",
        "        return best_ft_acc\n",
        "\n",
        "    # Save full model with hyperparameters\n",
        "    save_full_model(final_model, best_params, FULL_CKPT_PATH)\n",
        "\n",
        "    print(\"Transfer Learning Guide\")\n",
        "\n",
        "    print(\"\"\"\n",
        "        To use this model on other datasets:\n",
        "        \n",
        "        1. Similiar Datasets (e.g., CIFAR-100, STL-10, Tiny ImageNet):\n",
        "           - Load pretrained model with freeze_backbone=False\n",
        "           - Fine-tune entire model with lower learning rate\n",
        "        \n",
        "        2. Differents Domains (e.g., Medical images, Satellite imagery):\n",
        "           - Load pretrained model with freeze_backbone=True\n",
        "           - Train transformer layers and head first\n",
        "        \n",
        "        3. Different Image Sizes:\n",
        "           - Resize images to 32x32 OR\n",
        "           - Modify patch_embed layer for new resolution and let pos-enc interpolate\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"Files Saved:\")\n",
        "    print(f\"Best state dict       : {BEST_STATE_PATH}\")\n",
        "    print(f\"Full checkpoint (+hps) : {FULL_CKPT_PATH}\")\n",
        "    print(f\"Finetuned state dict   : {FINETUNED_PATH}\")\n",
        "    print(f\"Optuna best summary    : {STUDY_JSON_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a6c7ca6",
      "metadata": {},
      "source": [
        "## Ensemble Learning: Weighted Averaging (Logits) - Post-Training Experiments (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using device: cuda\n",
            "[INFO] Artifacts directory: /mnt/vit-hybrid-optuna\n",
            "[INFO] HybridCNNViT is already defined in this runtime.\n",
            "[WARN] Skipping HybridCNNViT because the class is unavailable.\n",
            "[INFO] Total ensemble members: 0\n",
            "[ERROR] No models are available for the ensemble.\n",
            "[ERROR] To include HybridCNNViT, either define the class earlier in the notebook, or set:\n",
            "[ERROR] os.environ['TRAIN_MODULE'] = 'train_hybrid'  # ensure train_hybrid.py defines HybridCNNViT and is on PYTHONPATH\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "No ensemble members found. Provide at least one valid checkpoint or define/import HybridCNNViT.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 269\u001b[39m\n\u001b[32m    267\u001b[39m     error(\u001b[33m\"\u001b[39m\u001b[33mTo include HybridCNNViT, either define the class earlier in the notebook, or set:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    268\u001b[39m     error(\u001b[33m\"\u001b[39m\u001b[33mos.environ[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTRAIN_MODULE\u001b[39m\u001b[33m'\u001b[39m\u001b[33m] = \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain_hybrid\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  # ensure train_hybrid.py defines HybridCNNViT and is on PYTHONPATH\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo ensemble members found. Provide at least one valid checkpoint or define/import HybridCNNViT.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[38;5;66;03m# Ensemble wrapper (logits)\u001b[39;00m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLogitsEnsemble\u001b[39;00m(nn.Module):\n",
            "\u001b[31mRuntimeError\u001b[39m: No ensemble members found. Provide at least one valid checkpoint or define/import HybridCNNViT."
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Ensemble Learning: Weighted Averaging (Logits)\n",
        "\n",
        "Purpose\n",
        "-------\n",
        "Build an ensemble classifier by combining logits from multiple models using\n",
        "weighted averaging. This implementation supports a HybridCNNViT member\n",
        "(if available) and optional ResNet-18 / ViT-Tiny members. It evaluates\n",
        "members and the ensemble on CIFAR-10 with torchmetrics and performs a small\n",
        "grid search to find the best weights by accuracy.\n",
        "\n",
        "Key behaviors\n",
        "-------------\n",
        "- Uses CIFAR-10 test set with standard normalization.\n",
        "- Tries to include HybridCNNViT from:\n",
        "  1) a class already defined in the current runtime, or\n",
        "  2) a module named in the TRAIN_MODULE environment variable (e.g., \"train_hybrid\").\n",
        "- Optionally includes ResNet-18 and ViT-Tiny if their checkpoints are present.\n",
        "- Evaluates metrics: accuracy, precision, recall, F1-macro.\n",
        "- Saves an ensemble_summary.json artifact with weights, metrics, and members.\n",
        "\n",
        "Assumptions\n",
        "-----------\n",
        "- Torch, Torchvision, Torchmetrics, and timm are installed.\n",
        "- CIFAR-10 will be downloaded if not found.\n",
        "\n",
        "Usage\n",
        "-----\n",
        "- Ensure HybridCNNViT is defined earlier in the notebook OR set:\n",
        "      os.environ[\"TRAIN_MODULE\"] = \"train_hybrid\"\n",
        "  and make sure train_hybrid.py defines `HybridCNNViT` and is on PYTHONPATH.\n",
        "- Optionally place checkpoints in MODEL_DIR (default: /mnt/vit-hybrid-optuna):\n",
        "    - hybrid_cnn_vit_full.pth OR both best_hybrid_cnn_vit.pth + optuna_best.json\n",
        "    - best_resnet18.pth (optional)\n",
        "    - best_vit_tiny.pth (optional)\n",
        "- Run this cell. It evaluates members, searches simple weights, prints results,\n",
        "  and writes ensemble_summary.json.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import List, Optional\n",
        "import importlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchmetrics import MetricCollection\n",
        "from torchmetrics.classification import (\n",
        "    MulticlassAccuracy, MulticlassPrecision, MulticlassRecall, MulticlassF1Score\n",
        ")\n",
        "import timm\n",
        "\n",
        "# Minimal logging helpers\n",
        "def _emit(tag: str, *objects, sep=\" \", end=\"\\n\", file=None, flush=False):\n",
        "    msg = sep.join(str(o) for o in objects)\n",
        "    # avoid double-tagging if caller already passed a tag\n",
        "    if msg.startswith((\"[INFO]\", \"[ERROR]\", \"[RESULT]\", \"[WARN]\")):\n",
        "        out = msg\n",
        "    else:\n",
        "        out = f\"{tag} {msg}\"\n",
        "    print(out, sep=\"\", end=end, file=file, flush=flush)\n",
        "\n",
        "def info(*a, **k):   _emit(\"[INFO]\",   *a, **k)\n",
        "def warn(*a, **k):   _emit(\"[WARN]\",   *a, **k)\n",
        "def error(*a, **k):  _emit(\"[ERROR]\",  *a, **k)\n",
        "def result(*a, **k): _emit(\"[RESULT]\", *a, **k)\n",
        "\n",
        "# Configuration and device\n",
        "MODEL_DIR = os.environ.get(\"MODEL_DIR\", \"/mnt/vit-hybrid-optuna\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "info(\"Using device:\", device)\n",
        "info(\"Artifacts directory:\", MODEL_DIR)\n",
        "\n",
        "# TorchMetrics setup\n",
        "try:\n",
        "    metric_collection  # noqa: F821\n",
        "    _metrics_ready = True\n",
        "except NameError:\n",
        "    _metrics_ready = False\n",
        "\n",
        "if not _metrics_ready:\n",
        "    NUM_CLASSES = 10  # CIFAR-10 by default; adjust for other datasets\n",
        "    AVG = \"macro\"\n",
        "    metric_collection = MetricCollection({\n",
        "        \"accuracy\":  MulticlassAccuracy(num_classes=NUM_CLASSES, average=AVG),\n",
        "        \"precision\": MulticlassPrecision(num_classes=NUM_CLASSES, average=AVG),\n",
        "        \"recall\":    MulticlassRecall(num_classes=NUM_CLASSES, average=AVG),\n",
        "        \"f1_score\":  MulticlassF1Score(num_classes=NUM_CLASSES, average=AVG),\n",
        "    }).to(device)\n",
        "    info(\"Initialized torchmetrics collection.\")\n",
        "\n",
        "# Utility: valid num_heads\n",
        "_CHOICES_HEADS = [4, 6, 8, 12]\n",
        "def _effective_num_heads_local(embed_dim: int, trial_choice: Optional[int]) -> int:\n",
        "    valid = [h for h in _CHOICES_HEADS if embed_dim % h == 0]\n",
        "    if not valid:\n",
        "        return 1\n",
        "    if trial_choice in valid:\n",
        "        return trial_choice  # type: ignore[arg-type]\n",
        "    return max(valid)\n",
        "\n",
        "# CIFAR-10 test loader\n",
        "def _get_testloader(batch_size: int = 128):\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    data_root = os.path.join(MODEL_DIR, \"data\")\n",
        "    try:\n",
        "        ds = datasets.CIFAR10(root=data_root, train=False, download=False, transform=tf)\n",
        "    except RuntimeError:\n",
        "        warn(\"CIFAR-10 test set not found in artifacts volume. Attempting download.\")\n",
        "        ds = datasets.CIFAR10(root=data_root, train=False, download=True, transform=tf)\n",
        "\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return dl, ds.classes\n",
        "\n",
        "# Try to locate HybridCNNViT\n",
        "HybridCNNViT = None  # will be assigned if found\n",
        "\n",
        "# If already defined earlier, keep it\n",
        "try:\n",
        "    _ = HybridCNNViT  # type: ignore # noqa: F821\n",
        "    info(\"HybridCNNViT is already defined in this runtime.\")\n",
        "except NameError:\n",
        "    # Try import from TRAIN_MODULE if provided\n",
        "    train_module_name = os.environ.get(\"TRAIN_MODULE\", None)\n",
        "    if train_module_name:\n",
        "        try:\n",
        "            train_module = importlib.import_module(train_module_name)\n",
        "            HybridCNNViT = getattr(train_module, \"HybridCNNViT\", None)\n",
        "            if HybridCNNViT is None:\n",
        "                raise AttributeError(\"HybridCNNViT not found in the provided training module.\")\n",
        "            info(f\"Imported HybridCNNViT from module '{train_module_name}'.\")\n",
        "        except Exception as e:\n",
        "            error(\"Failed to import HybridCNNViT from TRAIN_MODULE. Details:\", e)\n",
        "            HybridCNNViT = None\n",
        "    else:\n",
        "        warn(\"HybridCNNViT is not defined and TRAIN_MODULE is not set. Hybrid member will be skipped.\")\n",
        "        HybridCNNViT = None\n",
        "\n",
        "# Model builders/loaders\n",
        "FULL_CKPT_PATH = os.path.join(MODEL_DIR, \"hybrid_cnn_vit_full.pth\")\n",
        "BEST_STATE_PATH = os.path.join(MODEL_DIR, \"best_hybrid_cnn_vit.pth\")\n",
        "STUDY_JSON_PATH = os.path.join(MODEL_DIR, \"optuna_best.json\")\n",
        "\n",
        "def load_hybrid_from_full(full_ckpt_path: str) -> nn.Module:\n",
        "    if HybridCNNViT is None:\n",
        "        raise RuntimeError(\"HybridCNNViT class is unavailable; cannot load 'full' checkpoint.\")\n",
        "    ckpt = torch.load(full_ckpt_path, map_location=device)\n",
        "    hp = ckpt[\"hyperparameters\"]\n",
        "    heads_eff = _effective_num_heads_local(hp[\"embed_dim\"], hp.get(\"num_heads\"))\n",
        "    m = HybridCNNViT(\n",
        "        backbone=hp[\"backbone\"],\n",
        "        embed_dim=hp[\"embed_dim\"],\n",
        "        num_heads=heads_eff,\n",
        "        num_layers=hp[\"num_layers\"],\n",
        "        mlp_ratio=hp[\"mlp_ratio\"],\n",
        "        dropout=hp[\"dropout\"],\n",
        "        num_classes=ckpt[\"training_info\"][\"num_classes\"],\n",
        "    ).to(device).eval()\n",
        "    m.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "    return m\n",
        "\n",
        "def load_hybrid_from_best(best_state_path: str, hp_json: str) -> nn.Module:\n",
        "    if HybridCNNViT is None:\n",
        "        raise RuntimeError(\"HybridCNNViT class is unavailable; cannot load 'best' checkpoint.\")\n",
        "    with open(hp_json, \"r\") as f:\n",
        "        hp = json.load(f)[\"best_params\"]\n",
        "    heads_eff = _effective_num_heads_local(hp[\"embed_dim\"], hp.get(\"num_heads\"))\n",
        "    m = HybridCNNViT(\n",
        "        backbone=hp[\"backbone\"],\n",
        "        embed_dim=hp[\"embed_dim\"],\n",
        "        num_heads=heads_eff,\n",
        "        num_layers=hp[\"num_layers\"],\n",
        "        mlp_ratio=hp[\"mlp_ratio\"],\n",
        "        dropout=hp[\"dropout\"],\n",
        "        num_classes=10,  # adjust if your dataset differs\n",
        "    ).to(device).eval()\n",
        "    m.load_state_dict(torch.load(best_state_path, map_location=device))\n",
        "    return m\n",
        "\n",
        "def build_resnet18(num_classes: int = 10) -> nn.Module:\n",
        "    from torchvision.models import resnet18, ResNet18_Weights\n",
        "    m = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    return m.to(device).eval()\n",
        "\n",
        "def build_vit_tiny(num_classes: int = 10) -> nn.Module:\n",
        "    m = timm.create_model(\"vit_tiny_patch16_224\", pretrained=True, num_classes=num_classes)\n",
        "    # Optional adaptation for small images if that matches your training pipeline\n",
        "    m.patch_embed.patch_size = (4, 4)\n",
        "    warn(\"Using ViT-Tiny with patch_size=4 to adapt to small images.\")\n",
        "    return m.to(device).eval()\n",
        "\n",
        "# Collect ensemble members\n",
        "models: List[nn.Module] = []\n",
        "\n",
        "# HybridCNNViT (if available and checkpoints exist)\n",
        "if HybridCNNViT is not None and os.path.exists(FULL_CKPT_PATH):\n",
        "    info(\"Loading HybridCNNViT from FULL_CKPT_PATH:\", FULL_CKPT_PATH)\n",
        "    try:\n",
        "        models.append(load_hybrid_from_full(FULL_CKPT_PATH))\n",
        "    except Exception as e:\n",
        "        error(\"Failed to load HybridCNNViT from FULL_CKPT_PATH. Details:\", e)\n",
        "\n",
        "elif HybridCNNViT is not None and os.path.exists(BEST_STATE_PATH) and os.path.exists(STUDY_JSON_PATH):\n",
        "    info(\"Loading HybridCNNViT from BEST_STATE_PATH:\", BEST_STATE_PATH)\n",
        "    try:\n",
        "        models.append(load_hybrid_from_best(BEST_STATE_PATH, STUDY_JSON_PATH))\n",
        "    except Exception as e:\n",
        "        error(\"Failed to load HybridCNNViT from BEST_STATE_PATH. Details:\", e)\n",
        "else:\n",
        "    if HybridCNNViT is None:\n",
        "        warn(\"Skipping HybridCNNViT because the class is unavailable.\")\n",
        "    else:\n",
        "        warn(\"HybridCNNViT checkpoints not found. Skipping Hybrid member.\")\n",
        "\n",
        "# Optional members (if checkpoints exist)\n",
        "RESNET_PTH = os.path.join(MODEL_DIR, \"best_resnet18.pth\")\n",
        "VIT_PTH    = os.path.join(MODEL_DIR, \"best_vit_tiny.pth\")\n",
        "\n",
        "if os.path.exists(RESNET_PTH):\n",
        "    info(\"Loading ResNet-18 member from:\", RESNET_PTH)\n",
        "    try:\n",
        "        m_res = build_resnet18(num_classes=10)\n",
        "        m_res.load_state_dict(torch.load(RESNET_PTH, map_location=device))\n",
        "        models.append(m_res.eval())\n",
        "    except Exception as e:\n",
        "        error(\"Failed to load ResNet-18. Details:\", e)\n",
        "\n",
        "if os.path.exists(VIT_PTH):\n",
        "    info(\"Loading ViT-Tiny member from:\", VIT_PTH)\n",
        "    try:\n",
        "        m_vit = build_vit_tiny(num_classes=10)\n",
        "        m_vit.load_state_dict(torch.load(VIT_PTH, map_location=device))\n",
        "        models.append(m_vit.eval())\n",
        "    except Exception as e:\n",
        "        error(\"Failed to load ViT-Tiny. Details:\", e)\n",
        "\n",
        "info(\"Total ensemble members:\", len(models))\n",
        "for i, m in enumerate(models, start=1):\n",
        "    info(f\"  Model {i}: {type(m).__name__}\")\n",
        "\n",
        "if len(models) == 0:\n",
        "    error(\"No models are available for the ensemble.\")\n",
        "    error(\"To include HybridCNNViT, either define the class earlier in the notebook, or set:\")\n",
        "    error(\"os.environ['TRAIN_MODULE'] = 'train_hybrid'  # ensure train_hybrid.py defines HybridCNNViT and is on PYTHONPATH\")\n",
        "    raise RuntimeError(\"No ensemble members found. Provide at least one valid checkpoint or define/import HybridCNNViT.\")\n",
        "\n",
        "# Ensemble wrapper (logits)\n",
        "class LogitsEnsemble(nn.Module):\n",
        "    def __init__(self, models: List[nn.Module], weights: Optional[List[float]] = None):\n",
        "        super().__init__()\n",
        "        self.models = nn.ModuleList(models)\n",
        "        if weights is None:\n",
        "            weights = [1.0 / len(models)] * len(models)\n",
        "        s = sum(weights)\n",
        "        self.register_buffer(\"weights\", torch.tensor([w / s for w in weights], dtype=torch.float32))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        logits_sum = None\n",
        "        for w, m in zip(self.weights, self.models):\n",
        "            out = m(x)\n",
        "            logits_sum = w * out if logits_sum is None else logits_sum + w * out\n",
        "        return logits_sum\n",
        "\n",
        "# Evaluation with metrics\n",
        "@torch.no_grad()\n",
        "def eval_with_metrics(model: nn.Module, loader: DataLoader) -> dict:\n",
        "    model.eval()\n",
        "    metric_collection.reset()\n",
        "    total_loss, n = 0.0, 0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        total_loss += loss.item()\n",
        "        metric_collection.update(logits, yb)\n",
        "        n += 1\n",
        "    m = metric_collection.compute()\n",
        "    out = {k: v.item() for k, v in m.items()}\n",
        "    out[\"loss\"] = total_loss / max(1, n)\n",
        "    out[\"accuracy\"] *= 100.0  # percentage\n",
        "    return out\n",
        "\n",
        "# Grid search over weights\n",
        "def simple_weight_search(models: List[nn.Module], step: float = 0.1):\n",
        "    info(\"Loading test data for weight search.\")\n",
        "    testloader, class_names = _get_testloader()\n",
        "    n_models = len(models)\n",
        "\n",
        "    info(\"Evaluating individual models.\")\n",
        "    indiv_stats = []\n",
        "    for i, m in enumerate(models, start=1):\n",
        "        stats = eval_with_metrics(m, testloader)\n",
        "        info(f\"  Model {i} ({type(m).__name__}) accuracy: {stats['accuracy']:.2f}%\")\n",
        "        indiv_stats.append(stats)\n",
        "\n",
        "    if n_models == 1:\n",
        "        warn(\"Only one model available. Returning weight [1.0].\")\n",
        "        return [1.0], indiv_stats[0], class_names\n",
        "\n",
        "    best_w, best_stats, best_acc = None, None, -1.0\n",
        "    grid = [round(i * step, 2) for i in range(int(1 / step) + 1)]\n",
        "    info(f\"Starting weight search (n_models={n_models}, step={step}).\")\n",
        "\n",
        "    if n_models == 2:\n",
        "        for w0 in grid:\n",
        "            weights = [w0, round(1.0 - w0, 2)]\n",
        "            ens = LogitsEnsemble(models, weights).to(device)\n",
        "            stats = eval_with_metrics(ens, testloader)\n",
        "            if stats[\"accuracy\"] > best_acc:\n",
        "                best_acc, best_w, best_stats = stats[\"accuracy\"], weights, stats\n",
        "    elif n_models == 3:\n",
        "        for w0 in grid:\n",
        "            for w1 in [x for x in grid if x <= round(1.0 - w0, 2)]:\n",
        "                w2 = round(1.0 - w0 - w1, 2)\n",
        "                weights = [w0, w1, w2]\n",
        "                ens = LogitsEnsemble(models, weights).to(device)\n",
        "                stats = eval_with_metrics(ens, testloader)\n",
        "                if stats[\"accuracy\"] > best_acc:\n",
        "                    best_acc, best_w, best_stats = stats[\"accuracy\"], weights, stats\n",
        "    else:\n",
        "        warn(\"Weight search for n > 3 is not implemented. Using simple average.\")\n",
        "        weights = [1.0 / n_models] * n_models\n",
        "        ens = LogitsEnsemble(models, weights).to(device)\n",
        "        stats = eval_with_metrics(ens, testloader)\n",
        "        best_w, best_stats = weights, stats\n",
        "\n",
        "    return best_w, best_stats, class_names\n",
        "\n",
        "# Run ensemble search & report\n",
        "weights, stats, class_names = simple_weight_search(models, step=0.1)\n",
        "\n",
        "result(\"Ensemble weights:\", weights)\n",
        "result(f\"Accuracy: {stats['accuracy']:.2f}%\")\n",
        "result(f\"F1-Macro: {stats['f1_score']:.4f}\")\n",
        "result(f\"Precision: {stats['precision']:.4f}\")\n",
        "result(f\"Recall: {stats['recall']:.4f}\")\n",
        "result(f\"Loss: {stats['loss']:.4f}\")\n",
        "\n",
        "# Save artifacts\n",
        "ENSEMBLE_JSON = os.path.join(MODEL_DIR, \"ensemble_summary.json\")\n",
        "try:\n",
        "    with open(ENSEMBLE_JSON, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"weights\": weights,\n",
        "            \"metrics\": stats,\n",
        "            \"members\": [type(m).__name__ for m in models]\n",
        "        }, f, indent=2)\n",
        "    result(\"Ensemble summary saved to:\", ENSEMBLE_JSON)\n",
        "except Exception as e:\n",
        "    error(\"Failed to save ensemble summary. Details:\", e)\n",
        "\n",
        "# Optional: single-image API\n",
        "@torch.no_grad()\n",
        "def predict_image_ensemble(pil_image, ensemble_weights=None, class_names=class_names):\n",
        "    \"\"\"\n",
        "    Predict a single PIL image with the current ensemble.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    pil_image : PIL.Image.Image\n",
        "        The input image.\n",
        "    ensemble_weights : list[float] or None\n",
        "        Optional custom weights to override the best weights found.\n",
        "    class_names : list[str]\n",
        "        Class labels corresponding to the model outputs.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        {\"label\": str, \"index\": int, \"confidence\": float}\n",
        "    \"\"\"\n",
        "    global models, weights\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    x = tf(pil_image).unsqueeze(0).to(device)\n",
        "    active_weights = ensemble_weights or weights\n",
        "    ens = LogitsEnsemble(models, active_weights).to(device)\n",
        "    logits = ens(x)\n",
        "    prob = logits.softmax(1)[0]\n",
        "    idx = int(prob.argmax().item())\n",
        "    conf = float(prob.max().item())\n",
        "    label = class_names[idx] if class_names else str(idx)\n",
        "    return {\"label\": label, \"index\": idx, \"confidence\": conf}\n",
        "\n",
        "info(\"Helper 'predict_image_ensemble' is ready.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gemastik",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
